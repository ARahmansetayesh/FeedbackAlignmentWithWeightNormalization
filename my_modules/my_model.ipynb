{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "boBVVjYw41vK"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4hS4h78JO3V"
      },
      "source": [
        "# Model  $f(\\cdot)=Relu(tanh((\\cdot) + bias))$\n",
        "\n",
        "a model for implementig BP-TRW and BP for experiments of figures 6, 7, and 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYbIhxXfo8ZX"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# bias = False : Initial bias vectors with zero vectors\n",
        "# bias = True : Initial bias vectors with N(initial_mean , initial_var )\n",
        "# N_layer_list : A list which contains number of neurons in each layer. For example N_layer_list = [100,20,30]  creates a network with input layer of size 100, a single hidden layer of size 20, and output layer of size 30\n",
        "# Forward and backward weight matrices are initialized from N(initial_mean,initial_var )\n",
        "\n",
        "class my_network1():\n",
        "\n",
        "    def __init__(self ,  N_layer_list , bias = False , initial_mean = 0 ,  initial_var = 0.1):\n",
        "        self.norm_W_list = []\n",
        "        self.norm_B_list = []\n",
        "        self.bias_vec = []\n",
        "        self.N_layer_list = N_layer_list\n",
        "\n",
        "        self.W =[]\n",
        "        self.L =[]\n",
        "        self.Z =[]\n",
        "        self.delta_BP =[]\n",
        "        self.delta_FA =[]\n",
        "\n",
        "        self.E_BP =[]\n",
        "        self.E_FA =[]\n",
        "\n",
        "\n",
        "        self.B =[]\n",
        "\n",
        "        self.DeltaW_bp = []\n",
        "        self.DeltaW_fa = []\n",
        "\n",
        "        self.Delta_bias_bp = []\n",
        "        self.Delta_bias_fa = []\n",
        "\n",
        "        self.bias_vec.append(None)\n",
        "        self.Delta_bias_bp.append(None)\n",
        "        self.Delta_bias_fa.append(None)\n",
        "\n",
        "\n",
        "\n",
        "        self.num_layers  = len(self.N_layer_list)-1  \n",
        "        for i in range( self.num_layers ):\n",
        "\n",
        "          self.W.append( ((torch.randn(  N_layer_list[i]  , N_layer_list[i+1] )) ) .to(device).to(dtype)  * initial_var + initial_mean )\n",
        "\n",
        "\n",
        "          self.DeltaW_bp.append( torch.zeros_like(self.W[-1]).to(device) )\n",
        "          self.DeltaW_fa.append( torch.zeros_like(self.W[-1]).to(device) )\n",
        "          \n",
        "          self.B.append( ((torch.randn(self.N_layer_list[i] , self.N_layer_list[i+1]))  ).t().to(device).to(dtype)   * initial_var  + initial_mean   ) \n",
        "\n",
        "          if bias==True :\n",
        "             self.bias_vec.append (  torch.randn( 1  , self.N_layer_list[i+1] ) .to(device).to(dtype)   * initial_var  + initial_mean )\n",
        "          elif bias==False :\n",
        "             self.bias_vec.append (  torch.zeros( 1  , self.N_layer_list[i+1] ) .to(device).to(dtype)    )\n",
        "\n",
        "          self.Delta_bias_bp.append( torch.zeros( 1  , self.N_layer_list[i+1] ).to(device).to(dtype)  )\n",
        "          self.Delta_bias_fa.append( torch.zeros( 1  , self.N_layer_list[i+1] ) .to(device).to(dtype)  )\n",
        "\n",
        "\n",
        "          \n",
        "        for i in range( self.num_layers + 1 ):\n",
        "          self.L.append( None )\n",
        "          self.Z.append( None )\n",
        "          self.delta_FA.append( None  )\n",
        "          self.delta_BP.append( None  )\n",
        "\n",
        "          self.E_FA.append( None )\n",
        "          self.E_BP.append( None )\n",
        "\n",
        "\n",
        "    def forward( self , X ):\n",
        "        self.L[0] = X\n",
        "        for i in range( self.num_layers  ):\n",
        "          self.Z[i+1] = torch.matmul( self.L[i] , self.W[i] ) +  self.bias_vec [i+1]\n",
        "          self.L[i+1] = self.activation1( self.Z[i+1] ) \n",
        "        return self.L[-1]\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def backprop(self, E_l ):\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "        self.E_BP[-1] = E_l\n",
        "        self.delta_BP[-1] =  torch.mul( self.derivative_activation1( self.Z[-1] ) ,  self.E_BP[-1]  )\n",
        "      \n",
        "\n",
        "        for i in range(len(self.N_layer_list)-2):\n",
        "     \n",
        "          self.E_BP[ -2 - i] = torch.matmul(   self.delta_BP[-1-i] , torch.transpose( self.W[-1-i] , 0,1)   )\n",
        "          self.delta_BP[-2-i] =  torch.mul (  self.derivative_activation1(self.Z[-2-i] ) , self.E_BP[ -2 - i] )\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(len(self.N_layer_list)-1):\n",
        "          self.DeltaW_bp[i] = torch.matmul(   torch.transpose( self.L[i]  , 0,1) , self.delta_BP[i+1]    )  \n",
        "           \n",
        "        for i in range(1,len(self.N_layer_list)):\n",
        "          self.Delta_bias_bp[i] = self.delta_BP[i] . sum(dim=0) .view(1,-1)\n",
        "\n",
        "        return self.DeltaW_bp , self.Delta_bias_bp\n",
        "\n",
        "\n",
        "\n",
        "    def BP_TRW(self,  E ):\n",
        "\n",
        "\n",
        "  \n",
        "        self.E_FA[-1 ] = E\n",
        "\n",
        "\n",
        "\n",
        "        self.delta_FA[-1] =  torch.mul( self.derivative_activation1( self.Z[-1] ) ,  self.E_FA[-1 ]   )\n",
        " \n",
        "\n",
        "        for i in range(len(self.N_layer_list)-2):\n",
        "          self.E_FA[ -2 - i] = torch.matmul(   self.delta_FA[-1-i]   ,  self.B[-1-i]    )\n",
        "       \n",
        "          self.delta_FA[-2-i] =  torch.mul (self.derivative_activation1(self.Z[-2-i] ) , self.E_FA[ -2 - i ]     )\n",
        "      \n",
        "    \n",
        "\n",
        "\n",
        "        for i in range(len(self.N_layer_list)-1):\n",
        "\n",
        "          self.DeltaW_fa[i] = torch.matmul(   torch.transpose( self.L[i]  , 0,1) , self.delta_FA[i+1]    )  \n",
        "           \n",
        "        for i in range(1,len(self.N_layer_list)):\n",
        "          self.Delta_bias_fa[i] = self.delta_FA[i] . sum(dim=0) .view(1,-1)\n",
        "\n",
        "        return self.DeltaW_fa , self.Delta_bias_fa\n",
        "\n",
        "\n",
        "\n",
        "    def activation1(self,x):\n",
        " \n",
        "\n",
        "      return torch.tanh(torch.relu(x))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def derivative_activation1(self,x):\n",
        "\n",
        "\n",
        "\n",
        "      m = (x > 0) * 1\n",
        "\n",
        "      x = torch.cosh(x)\n",
        "      x = torch.mul ( x , x )\n",
        "      x = 1/x\n",
        "\n",
        "      return torch.mul ( x , m )\n",
        "      \n",
        "\n",
        "\n",
        "    def set_learning_rate(self , lr):\n",
        "      self.lr = lr\n",
        "\n",
        "\n",
        "\n",
        "    def step_W(self,deltaW):\n",
        "      for i in range(len(self.N_layer_list)-1):\n",
        "        self.W[i] = torch.add( self.W[i] , deltaW[i] ,alpha= self.lr )\n",
        "\n",
        "    \n",
        "\n",
        "    def step_bias(self,delta_bias):\n",
        "      for i in range(1,len(self.N_layer_list)):\n",
        "        self.bias_vec[i] = torch.add( self.bias_vec[i] , delta_bias[i] ,alpha= self.lr )\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "    def normalize_W(self , norm = None ):\n",
        "      if norm==None:\n",
        "        for i in range(len(self.N_layer_list)-1):\n",
        "          self.W[i] = self.W[i] / self.W[i].norm() * self.norm_W_list[i]\n",
        "      elif type(norm)==list :   \n",
        "        for i in range(len(self.N_layer_list)-1):\n",
        "          self.W[i] = self.W[i] / self.W[i].norm() * norm  [i]\n",
        "      else : \n",
        "        for i in range(len(self.N_layer_list)-1):\n",
        "          self.W[i] = self.W[i] / self.W[i].norm() * norm\n",
        "\n",
        "\n",
        "    def normalize_B(self , norm = None ):\n",
        "      if norm==None:\n",
        "        for i in range(len(self.N_layer_list)-1):\n",
        "          self.B[i] = self.B[i] / self.B[i].norm() * self.norm_B_list[i]\n",
        "      elif type(norm)==list :   \n",
        "        for i in range(len(self.N_layer_list)-1):\n",
        "          self.B[i] = self.B[i] / self.B[i].norm() * norm  [i]          \n",
        "      else : \n",
        "        for i in range(len(self.N_layer_list)-1):\n",
        "          self.B[i] = self.B[i] / self.B[i].norm() * norm\n",
        "\n",
        "\n",
        "    def match_B_norm_to_W_norm(self):\n",
        "      for i in range(len(self.N_layer_list)-1):\n",
        "        self.B[i] = self.B[i] / self.B[i].norm() *self.W[i].norm() \n",
        "\n",
        "\n",
        "    def save_norm( self ):\n",
        "      self.norm_W_list = []\n",
        "      self.norm_B_list = []\n",
        "      for i in range(len(self.N_layer_list)-1):\n",
        "\n",
        "        self.norm_W_list . append( self.W[i] .norm() )\n",
        "        self.norm_B_list . append( self.B[i] .norm() )\n",
        "\n",
        "\n",
        "    \n",
        "    def column_normalize_W(self , norm = None ):\n",
        "        if norm==None:\n",
        "          for l in range(len(self.N_layer_list)-1):\n",
        "            self.W[l] = torch.mul( torch.div( self.W[l] ,  self.W[l].norm(dim=0).view( [1,-1] )  )  , self.norm_W_list[l].view( [1,-1] ) )\n",
        "        elif type(norm)==list : \n",
        "          for l in range(len(self.N_layer_list)-1):\n",
        "            self.W[l] = torch.div( self.W[l] ,  self.W[l].norm(dim=0).view( [1,-1] )  )  * norm[l]\n",
        "        else : \n",
        "          for l in range(len(self.N_layer_list)-1):\n",
        "            self.W[l] = torch.div( self.W[l] ,  self.W[l].norm(dim=0).view( [1,-1] )  )  * norm\n",
        "\n",
        "    def column_normalize_B(self , norm = None ):\n",
        "        if norm==None:\n",
        "          for l in range(len(self.N_layer_list)-1):\n",
        "            self.B[l] = torch.mul( torch.div( self.B[l] ,  self.B[l].norm(dim=0).view( [1,-1] )  ) , self.norm_B_list[l].view( [1,-1] ) )\n",
        "\n",
        "        elif type(norm)==list : \n",
        "          for l in range(len(self.N_layer_list)-1):\n",
        "            self.B[l] = torch.div( self.B[l] ,  self.B[l].norm(dim=0).view( [1,-1] )  )  * norm[l]\n",
        "        else : \n",
        "          for l in range(len(self.N_layer_list)-1):\n",
        "            self.B[l] = torch.div( self.B[l] ,  self.B[l].norm(dim=0).view( [1,-1] )  )  * norm\n",
        "  \n",
        "\n",
        "\n",
        "    def seed_norms( self , from1 , till1 ):\n",
        "      for l in range(len(self.N_layer_list)-1):\n",
        "        self.norm_W_list .append( torch.rand(  self.N_layer_list[l+1]   ) .to(device) .to(dtype) *( till1 - from1 ) + from1 )\n",
        "        self.norm_B_list .append( torch.rand(  self.N_layer_list[l]   ) .to(device).to(dtype) *( till1 - from1 ) + from1 )\n",
        "\n",
        "    def set_norm( self , norm ):\n",
        "      self.norm_W_list = []\n",
        "      self.norm_B_list = []\n",
        "      for i in range(len(self.N_layer_list)-1):\n",
        "        self.norm_W_list . append(norm )\n",
        "        self.norm_B_list . append(norm )\n",
        "\n",
        "\n",
        "    def set_norm_LIST( self , norm_LIST ):\n",
        "      self.norm_W_list = []\n",
        "      for i in range(len(self.N_layer_list)-1):\n",
        "        self.norm_W_list . append(norm_LIST[i] )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}